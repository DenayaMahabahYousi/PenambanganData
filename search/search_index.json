{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tugas Data Mining Nama : Denaya Mahabah Yousi NIM : 180411100098 Kelas : 5B","title":"Home"},{"location":"#tugas-data-mining","text":"Nama : Denaya Mahabah Yousi NIM : 180411100098 Kelas : 5B","title":"Tugas Data Mining"},{"location":"NaiveBayes/","text":"Naive Bayes Classifier Classifier adalah model machine learning yang digunakan untuk membedakan objek berdasarkan fitur tertentu. Naive Bayes Classifier adalah machine learning yang menggunakan probabilitas untuk mengklasfikasi objek dimana untuk setiap fitur X sejumlah n : from IPython.display import Image Image(\"img/nb1.png\") P adalah probabilitas yang muncul. Untuk data numerik P adalah: from IPython.display import Image Image(\"img/nb2.png\") dimana v adalah nilai dalam fitur, sigma k adalah Standar deviasi dan mu k adalah Rata-rata untuk K (kolom) Langkah-Langkah Training 1. Ambil data set from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa 2. Sampel data untuk di tes test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4] 3. Identifikasi Per Grup Class Target untuk data Training dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica 5. Hitung Probabilitas Prior dan Likehood WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4 6. Rank & Tarik Kesimpulan Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica Kesimpulan corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Tugas 4 - Naive Bayes"},{"location":"NaiveBayes/#naive-bayes-classifier","text":"Classifier adalah model machine learning yang digunakan untuk membedakan objek berdasarkan fitur tertentu. Naive Bayes Classifier adalah machine learning yang menggunakan probabilitas untuk mengklasfikasi objek dimana untuk setiap fitur X sejumlah n : from IPython.display import Image Image(\"img/nb1.png\") P adalah probabilitas yang muncul. Untuk data numerik P adalah: from IPython.display import Image Image(\"img/nb2.png\") dimana v adalah nilai dalam fitur, sigma k adalah Standar deviasi dan mu k adalah Rata-rata untuk K (kolom)","title":"Naive Bayes Classifier"},{"location":"NaiveBayes/#langkah-langkah-training","text":"","title":"Langkah-Langkah Training"},{"location":"NaiveBayes/#1-ambil-data-set","text":"from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa","title":"1. Ambil data set"},{"location":"NaiveBayes/#2-sampel-data-untuk-di-tes","text":"test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4]","title":"2. Sampel data untuk di tes"},{"location":"NaiveBayes/#3-identifikasi-per-grup-class-target-untuk-data-training","text":"dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica","title":"3. Identifikasi Per Grup Class Target untuk data Training"},{"location":"NaiveBayes/#5-hitung-probabilitas-prior-dan-likehood","text":"WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4","title":"5. Hitung Probabilitas Prior dan Likehood"},{"location":"NaiveBayes/#6-rank-tarik-kesimpulan","text":"Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica","title":"6. Rank &amp; Tarik Kesimpulan"},{"location":"NaiveBayes/#kesimpulan","text":"corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Kesimpulan"},{"location":"Tugas1/","text":"Tugas 1 Data Mining : Pengantar Data Mining Pengertian Data Mining Data Mining adalah proses yang menggunakan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar (Turban dkk. 2005). Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu Knowledge discovery in databases (KDD), ekstraksi pengetahuan (knowledge extraction), Analisa data/pola (data/pattern analysis), kecerdasan bisnis (business intelligence) dan data archaeology dan data dredging(Larose, 2005) Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk : Prediksi trend dan sifat-sifat bisnis, dimana data mining mengotomatisasi proses pencarian informasi pemprediksi di dalam basis data yang besar. Penemuan pola-pola yang tidak diketahui sebelumnya, dimana data mining menyapu basis data, kemudian mengidentifikasi pola-pola yang sebelumnya tersembunyi dalam satu sapuan. Data mining berguna untuk membuat keputusan yang kritis, terutama dalam strategi. Berikut ini beberapa definisi data mining dari beberapa sumber (Larose, 2005): Data mining adalah proses menemukan sesuatu yang bermakna dari suatu korelasi baru, pola dan tren yang ada dengan cara memilah-milah data berukuran besar yang disimpan dalam repositori, menggunakan teknologi pengenalan pola serta teknik matematika dan statistik. Data mining adalah analisis pengamatan database untuk menemukan hubungan yang tidak terduga dan untuk meringkas data dengan cara atau metode baru yang dapat dimengerti dan bermanfaat kepada pemilik data. Data mining merupakan bidang ilmu interdisipliner yang menyatukan teknik pembelajaran dari mesin (machine learning), pengenalan pola (pattern recognition), statistik, database, dan visualisasi untuk mengatasi masalah ekstraksi informasi dari basis data yang besar. Data mining diartikan sebagai suatu proses ekstraksi informasi berguna dan potensial dari sekumpulan data yang terdapat secara implisit dalam suatu basis data. Fungsi Data Mining Data mining mempunyai fungsi yang penting untuk membantu mendapatkan informasi yang berguna serta meningkatkan pengetahuan bagi pengguna. Pada dasarnya, data mining mempunyai empat fungsi dasar yaitu: Fungsi Prediksi ( prediction ). Proses untuk menemukan pola dari data dengan menggunakan beberapa variabel untuk memprediksikan variabel lain yang tidak diketahui jenis atau nilainya. Fungsi Deskripsi ( description ). Proses untuk menemukan suatu karakteristik penting dari data dalam suatu basis data. Fungsi Klasifikasi ( classification ). Klasifikasi merupakan suatu proses untuk menemukan model atau fungsi untuk menggambarkan class atau konsep dari suatu data. Proses yang digunakan untuk mendeskripsikan data yang penting serta dapat meramalkan kecenderungan data pada masa depan. Fungsi Asosiasi ( association ). Proses ini digunakan untuk menemukan suatu hubungan yang terdapat pada nilai atribut dari sekumpulan data. Proses Data Mining Proses yang umumnya dilakukan oleh data mining antara lain: deskripsi, prediksi, estimasi, klasifikasi, clustering dan asosiasi. Secara rinci proses data mining dijelaskan sebagai berikut (Larose, 2005): a. Deskripsi Deskripsi bertujuan untuk mengidentifikasi pola yang muncul secara berulang pada suatu data dan mengubah pola tersebut menjadi aturan dan kriteria yang dapat mudah dimengerti oleh para ahli pada domain aplikasinya. Aturan yang dihasilkan harus mudah dimengerti agar dapat dengan efektif meningkatkan tingkat pengetahuan (knowledge) pada sistem. Tugas deskriptif merupakan tugas data mining yang sering dibutuhkan pada teknik postprocessing untuk melakukan validasi dan menjelaskan hasil dari proses data mining. Postprocessing merupakan proses yang digunakan untuk memastikan hanya hasil yang valid dan berguna yang dapat digunakan oleh pihak yang berkepentingan. b. Prediksi Prediksi memiliki kemiripan dengan klasifikasi, akan tetapi data diklasifikasikan berdasarkan perilaku atau nilai yang diperkirakan pada masa yang akan datang. Contoh dari tugas prediksi misalnya untuk memprediksikan adanya pengurangan jumlah pelanggan dalam waktu dekat dan prediksi harga saham dalam tiga bulan yang akan datang. c. Estimasi Estimasi hampir sama dengan prediksi, kecuali variabel target estimasi lebih ke arah numerik dari pada ke arah kategori. Model dibangun menggunakan record lengkap yang menyediakan nilai dari variabel target sebagai nilai prediksi. Selanjutnya, pada peninjauan berikutnya estimasi nilai dari variabel target dibuat berdasarkan nilai variabel prediksi. Sebagai contoh, akan dilakukan estimasi tekanan darah sistolik pada pasien rumah sakit berdasarkan umur pasien, jenis kelamin, berat badan, dan level sodium darah. Hubungan antara tekanan darah sistolik dan nilai variabel prediksi dalam proses pembelajaran akan menghasilkan model estimasi. d. Klasifikasi Klasifikasi merupakan proses menemukan sebuah model atau fungsi yang mendeskripsikan dan membedakan data ke dalam kelas-kelas. Klasifikasi melibatkan proses pemeriksaan karakteristik dari objek dan memasukkan objek ke dalam salah satu kelas yang sudah didefinisikan sebelumnya. e. Clustering Clustering merupakan pengelompokan data tanpa berdasarkan kelas data tertentu ke dalam kelas objek yang sama. Sebuah kluster adalah kumpulan record yang memiliki kemiripan suatu dengan yang lainnya dan memiliki ketidakmiripan dengan record dalam kluster lain. Tujuannya adalah untuk menghasilkan pengelompokan objek yang mirip satu sama lain dalam kelompok-kelompok. Semakin besar kemiripan objek dalam suatu cluster dan semakin besar perbedaan tiap cluster maka kualitas analisis cluster semakin baik. f. Asosiasi Tugas asosiasi dalam data mining adalah menemukan atribut yang muncul dalam suatu waktu. Dalam dunia bisnis lebih umum disebut analisis keranjang belanja (market basket analisys). Tugas asosiasi berusaha untuk mengungkap aturan untuk mengukur hubungan antara dua atau lebih atribut. Tahapan Data Mining Tahapan yang dilakukan pada proses data mining diawali dari seleksi data dari data sumber ke data target, tahap preprocessing untuk memperbaiki kualitas data, transformasi, data mining serta tahap interpretasi dan evaluasi yang menghasilkan output berupa pengetahuan baru yang diharapkan memberikan kontribusi yang lebih baik. Secara detail dijelaskan sebagai berikut (Fayyad, 1996): from IPython.display import Image Image(\"img/Tahapan-Data-Mining.png\") 1. Data selection Pemilihan (seleksi) data dari sekumpulan data operasional perlu dilakukan sebelum tahap penggalian informasi dalam KDD dimulai. Data hasil seleksi yang digunakan untuk proses data mining, disimpan dalam suatu berkas, terpisah dari basis data operasional. 2. Pre-processing / cleaning Sebelum proses data mining dapat dilaksanakan, perlu dilakukan proses cleaning pada data yang menjadi fokus KDD. Proses cleaning mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data. 3. Transformation Coding adalah proses transformasi pada data yang telah dipilih, sehingga data tersebut sesuai untuk proses data mining. Proses coding dalam KDD merupakan proses kreatif dan sangat tergantung pada jenis atau pola informasi yang akan dicari dalam basis data. 4. Data mining Data mining adalah proses mencari pola atau informasi menarik dalam data terpilih dengan menggunakan teknik atau metode tertentu. Teknik, metode, atau algoritma dalam data mining sangat bervariasi. Pemilihan metode atau algoritma yang tepat sangat bergantung pada tujuan dan proses KDD secara keseluruhan. 5. Interpretation / evalution Pola informasi yang dihasilkan dari proses data mining perlu ditampilkan dalam bentuk yang mudah dimengerti oleh pihak yang berkepentingan. Tahap ini merupakan bagian dari proses KDD yang disebut interpretation. Tahap ini mencakup pemeriksaan apakah pola atau informasi yang ditemukan bertentangan dengan fakta atau hipotesis yang ada sebelumnya. Statistik Deskriptif Mean Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : Image(\"img/mean.png\") Median Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: variansi merupakan salah satu ukuran sebaran yang paling sering digunakan dalam berbagai analisis statistika. Standar deviasi merupakan akar kuadrat positif dari variansi. Secara umum, variansi dirumuskun sabagai : Image(\"img/median.png\") Modus Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Sedangkan data ordinal adalah data kategorik yang bisa diurutkan, misalnya kita menanyakan kepada 100 orang tentang kebiasaan untuk mencuci kaki sebelum tidur, dengan pilihan jawaban: selalu (5), sering (4), kadang-kadang(3), jarang (2), tidak pernah (1). Apabila kita ingin melihat ukuran pemusatannya lebih baik menggunakan modus yaitu yaitu jawaban yang paling banyak dipilih, misalnya sering (2). Berarti sebagian besar orang dari 100 orang yang ditanyakan menjawab sering mencuci kaki sebelum tidur. Inilah cara menghitung modus: Image(\"img/modus.png\") Data yang belum dikelompokkan Modus dari data yang belum dikelompokkan adalah ukuran yang memiliki frekuensi tertinggi. Modus dilambangkan mo. Data yang telah dikelompokkan Rumus Modus dari data yang telah dikelompokkan dihitung dengan rumus: Standar deviasi Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual thd rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Image(\"img/std.png\") Skewness (kemiringan atau kecondongan) Kecondongan suatu kurva dapat dilihat dari perbedaan letak mean, median dan modusnya. Jika ketiga ukuran pemusatan data tersebut berada pada titik yang sama, maka dikatakan simetris atau data berdistribusi normal. Sedangkan jika tidak berarti data tidak simetris atau tidak berdistribusi normal. Ukuran kecondongan data terbagi atas tiga bagian, yaitu : Kecondongan data ke arah kiri (condong negatif) dimana nilai modus lebih dari nilai mean (modus > mean). Kecondongan data simetris (distribusi normal) dimana nilai mean dan modus adalah sama (mean = modus). Kecondongan data ke arah kanan (condong positif) dimana nilai mean lebih dari nilai modus (mean > modus). Image(\"img/skew.png\") Menampilkan data menggunakan modul-modul dari python import pandas as pd pd.read_csv(\"tinggi.csv\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TINGGI BADAN 0 177 1 169 2 171 3 173 4 172 5 173 6 170 7 172 8 169 9 166 10 176 11 172 12 175 13 169 14 173 15 176 16 173 17 172 18 175 19 177 20 167 21 166 22 176 23 170 24 179 25 167 26 168 27 173 28 177 29 180 ... ... 70 167 71 165 72 178 73 170 74 170 75 175 76 180 77 177 78 178 79 169 80 166 81 170 82 168 83 178 84 176 85 175 86 166 87 180 88 179 89 177 90 168 91 172 92 165 93 165 94 168 95 167 96 179 97 173 98 172 99 167 100 rows \u00d7 1 columns Menampilkan statistik deskriptif from scipy import stats df=pd.read_csv(\"tinggi.csv\") df['TINGGI BADAN'].describe() count 100.000000 mean 172.030000 std 4.370366 min 165.000000 25% 168.000000 50% 172.000000 75% 175.250000 max 180.000000 Name: TINGGI BADAN, dtype: float64 Skewness df['TINGGI BADAN'].skew() 0.11048890855968597 import seaborn as sns sns.distplot(df); C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg. warnings.warn(\"The 'normed' kwarg is deprecated, and has been \" Tugas Membuat data random sebanyak 4 kolom dan hitung statistik deskriptifnya. from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('data1.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Tinggi Badan (cm) Berat Badan (kg) Lebar Bahu (cm) Tk Darah (mm/Hg) 0 173 65 39 142 1 154 72 40 145 2 165 59 43 164 3 156 65 38 133 4 156 59 37 147 5 163 49 44 132 6 174 53 40 169 7 175 41 37 121 8 160 40 48 157 9 157 54 38 149 10 163 65 44 135 11 157 54 36 125 12 162 62 40 111 13 164 79 44 108 14 175 74 43 168 15 155 47 41 166 16 171 59 42 120 17 171 57 39 168 18 163 75 36 129 19 168 74 36 125 20 160 61 50 154 21 157 50 41 124 22 167 48 43 118 23 173 64 46 105 24 162 63 44 107 25 170 56 43 152 26 173 78 50 142 27 157 65 44 130 28 163 59 45 140 29 166 72 39 156 ... ... ... ... ... 70 171 68 41 101 71 165 45 47 120 72 160 61 46 130 73 166 61 45 109 74 170 74 38 103 75 173 55 35 163 76 166 63 35 137 77 166 70 38 108 78 166 60 37 120 79 171 63 47 105 80 173 76 37 104 81 159 50 44 139 82 155 75 50 168 83 169 58 46 118 84 171 41 46 108 85 161 65 42 152 86 163 43 49 130 87 161 44 40 132 88 160 56 41 119 89 163 61 45 169 90 163 63 35 153 91 154 45 46 150 92 157 58 49 147 93 158 67 47 117 94 160 52 47 102 95 167 41 44 134 96 168 66 49 142 97 161 49 38 116 98 159 65 49 161 99 175 41 46 157 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"stats\"]+[x for x in df.columns], [\"count\"]+[df[col].count() for col in df.columns], [\"mean\"]+[df[col].mean() for col in df.columns], [\"std\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min\"]+[df[col].min() for col in df.columns], [\"max\"]+[df[col].max() for col in df.columns], [\"q1\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) stats Tinggi Badan (cm) Berat Badan (kg) Lebar Bahu (cm) Tk Darah (mm/Hg) count 100 100 100 100 mean 164.98 59.53 42.19 132.15 std 6.17 10.66 4.42 19.99 min 154 40 35 101 max 175 80 50 169 q1 160.0 52.75 38.75 116.75 q2 165.5 60.0 42.5 132.0 q3 170.25 66.0 46.0 148.25 skew -0.03 -0.05 0.02 0.12","title":"Tugas 1 - Pengantar Data Mining"},{"location":"Tugas1/#tugas-1-data-mining-pengantar-data-mining","text":"","title":"Tugas 1 Data Mining : Pengantar Data Mining"},{"location":"Tugas1/#pengertian-data-mining","text":"Data Mining adalah proses yang menggunakan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar (Turban dkk. 2005). Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu Knowledge discovery in databases (KDD), ekstraksi pengetahuan (knowledge extraction), Analisa data/pola (data/pattern analysis), kecerdasan bisnis (business intelligence) dan data archaeology dan data dredging(Larose, 2005) Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk : Prediksi trend dan sifat-sifat bisnis, dimana data mining mengotomatisasi proses pencarian informasi pemprediksi di dalam basis data yang besar. Penemuan pola-pola yang tidak diketahui sebelumnya, dimana data mining menyapu basis data, kemudian mengidentifikasi pola-pola yang sebelumnya tersembunyi dalam satu sapuan. Data mining berguna untuk membuat keputusan yang kritis, terutama dalam strategi. Berikut ini beberapa definisi data mining dari beberapa sumber (Larose, 2005): Data mining adalah proses menemukan sesuatu yang bermakna dari suatu korelasi baru, pola dan tren yang ada dengan cara memilah-milah data berukuran besar yang disimpan dalam repositori, menggunakan teknologi pengenalan pola serta teknik matematika dan statistik. Data mining adalah analisis pengamatan database untuk menemukan hubungan yang tidak terduga dan untuk meringkas data dengan cara atau metode baru yang dapat dimengerti dan bermanfaat kepada pemilik data. Data mining merupakan bidang ilmu interdisipliner yang menyatukan teknik pembelajaran dari mesin (machine learning), pengenalan pola (pattern recognition), statistik, database, dan visualisasi untuk mengatasi masalah ekstraksi informasi dari basis data yang besar. Data mining diartikan sebagai suatu proses ekstraksi informasi berguna dan potensial dari sekumpulan data yang terdapat secara implisit dalam suatu basis data.","title":"Pengertian Data Mining"},{"location":"Tugas1/#fungsi-data-mining","text":"Data mining mempunyai fungsi yang penting untuk membantu mendapatkan informasi yang berguna serta meningkatkan pengetahuan bagi pengguna. Pada dasarnya, data mining mempunyai empat fungsi dasar yaitu: Fungsi Prediksi ( prediction ). Proses untuk menemukan pola dari data dengan menggunakan beberapa variabel untuk memprediksikan variabel lain yang tidak diketahui jenis atau nilainya. Fungsi Deskripsi ( description ). Proses untuk menemukan suatu karakteristik penting dari data dalam suatu basis data. Fungsi Klasifikasi ( classification ). Klasifikasi merupakan suatu proses untuk menemukan model atau fungsi untuk menggambarkan class atau konsep dari suatu data. Proses yang digunakan untuk mendeskripsikan data yang penting serta dapat meramalkan kecenderungan data pada masa depan. Fungsi Asosiasi ( association ). Proses ini digunakan untuk menemukan suatu hubungan yang terdapat pada nilai atribut dari sekumpulan data.","title":"Fungsi Data Mining"},{"location":"Tugas1/#proses-data-mining","text":"Proses yang umumnya dilakukan oleh data mining antara lain: deskripsi, prediksi, estimasi, klasifikasi, clustering dan asosiasi. Secara rinci proses data mining dijelaskan sebagai berikut (Larose, 2005):","title":"Proses Data Mining"},{"location":"Tugas1/#a-deskripsi","text":"Deskripsi bertujuan untuk mengidentifikasi pola yang muncul secara berulang pada suatu data dan mengubah pola tersebut menjadi aturan dan kriteria yang dapat mudah dimengerti oleh para ahli pada domain aplikasinya. Aturan yang dihasilkan harus mudah dimengerti agar dapat dengan efektif meningkatkan tingkat pengetahuan (knowledge) pada sistem. Tugas deskriptif merupakan tugas data mining yang sering dibutuhkan pada teknik postprocessing untuk melakukan validasi dan menjelaskan hasil dari proses data mining. Postprocessing merupakan proses yang digunakan untuk memastikan hanya hasil yang valid dan berguna yang dapat digunakan oleh pihak yang berkepentingan.","title":"a. Deskripsi"},{"location":"Tugas1/#b-prediksi","text":"Prediksi memiliki kemiripan dengan klasifikasi, akan tetapi data diklasifikasikan berdasarkan perilaku atau nilai yang diperkirakan pada masa yang akan datang. Contoh dari tugas prediksi misalnya untuk memprediksikan adanya pengurangan jumlah pelanggan dalam waktu dekat dan prediksi harga saham dalam tiga bulan yang akan datang.","title":"b. Prediksi"},{"location":"Tugas1/#c-estimasi","text":"Estimasi hampir sama dengan prediksi, kecuali variabel target estimasi lebih ke arah numerik dari pada ke arah kategori. Model dibangun menggunakan record lengkap yang menyediakan nilai dari variabel target sebagai nilai prediksi. Selanjutnya, pada peninjauan berikutnya estimasi nilai dari variabel target dibuat berdasarkan nilai variabel prediksi. Sebagai contoh, akan dilakukan estimasi tekanan darah sistolik pada pasien rumah sakit berdasarkan umur pasien, jenis kelamin, berat badan, dan level sodium darah. Hubungan antara tekanan darah sistolik dan nilai variabel prediksi dalam proses pembelajaran akan menghasilkan model estimasi.","title":"c. Estimasi"},{"location":"Tugas1/#d-klasifikasi","text":"Klasifikasi merupakan proses menemukan sebuah model atau fungsi yang mendeskripsikan dan membedakan data ke dalam kelas-kelas. Klasifikasi melibatkan proses pemeriksaan karakteristik dari objek dan memasukkan objek ke dalam salah satu kelas yang sudah didefinisikan sebelumnya.","title":"d. Klasifikasi"},{"location":"Tugas1/#e-clustering","text":"Clustering merupakan pengelompokan data tanpa berdasarkan kelas data tertentu ke dalam kelas objek yang sama. Sebuah kluster adalah kumpulan record yang memiliki kemiripan suatu dengan yang lainnya dan memiliki ketidakmiripan dengan record dalam kluster lain. Tujuannya adalah untuk menghasilkan pengelompokan objek yang mirip satu sama lain dalam kelompok-kelompok. Semakin besar kemiripan objek dalam suatu cluster dan semakin besar perbedaan tiap cluster maka kualitas analisis cluster semakin baik.","title":"e. Clustering"},{"location":"Tugas1/#f-asosiasi","text":"Tugas asosiasi dalam data mining adalah menemukan atribut yang muncul dalam suatu waktu. Dalam dunia bisnis lebih umum disebut analisis keranjang belanja (market basket analisys). Tugas asosiasi berusaha untuk mengungkap aturan untuk mengukur hubungan antara dua atau lebih atribut.","title":"f. Asosiasi"},{"location":"Tugas1/#tahapan-data-mining","text":"Tahapan yang dilakukan pada proses data mining diawali dari seleksi data dari data sumber ke data target, tahap preprocessing untuk memperbaiki kualitas data, transformasi, data mining serta tahap interpretasi dan evaluasi yang menghasilkan output berupa pengetahuan baru yang diharapkan memberikan kontribusi yang lebih baik. Secara detail dijelaskan sebagai berikut (Fayyad, 1996): from IPython.display import Image Image(\"img/Tahapan-Data-Mining.png\")","title":"Tahapan Data Mining"},{"location":"Tugas1/#1-data-selection","text":"Pemilihan (seleksi) data dari sekumpulan data operasional perlu dilakukan sebelum tahap penggalian informasi dalam KDD dimulai. Data hasil seleksi yang digunakan untuk proses data mining, disimpan dalam suatu berkas, terpisah dari basis data operasional.","title":"1. Data selection"},{"location":"Tugas1/#2-pre-processing-cleaning","text":"Sebelum proses data mining dapat dilaksanakan, perlu dilakukan proses cleaning pada data yang menjadi fokus KDD. Proses cleaning mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data.","title":"2. Pre-processing / cleaning"},{"location":"Tugas1/#3-transformation","text":"Coding adalah proses transformasi pada data yang telah dipilih, sehingga data tersebut sesuai untuk proses data mining. Proses coding dalam KDD merupakan proses kreatif dan sangat tergantung pada jenis atau pola informasi yang akan dicari dalam basis data.","title":"3. Transformation"},{"location":"Tugas1/#4-data-mining","text":"Data mining adalah proses mencari pola atau informasi menarik dalam data terpilih dengan menggunakan teknik atau metode tertentu. Teknik, metode, atau algoritma dalam data mining sangat bervariasi. Pemilihan metode atau algoritma yang tepat sangat bergantung pada tujuan dan proses KDD secara keseluruhan.","title":"4. Data mining"},{"location":"Tugas1/#5-interpretation-evalution","text":"Pola informasi yang dihasilkan dari proses data mining perlu ditampilkan dalam bentuk yang mudah dimengerti oleh pihak yang berkepentingan. Tahap ini merupakan bagian dari proses KDD yang disebut interpretation. Tahap ini mencakup pemeriksaan apakah pola atau informasi yang ditemukan bertentangan dengan fakta atau hipotesis yang ada sebelumnya.","title":"5. Interpretation / evalution"},{"location":"Tugas1/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"Tugas1/#mean","text":"Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : Image(\"img/mean.png\")","title":"Mean"},{"location":"Tugas1/#median","text":"Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: variansi merupakan salah satu ukuran sebaran yang paling sering digunakan dalam berbagai analisis statistika. Standar deviasi merupakan akar kuadrat positif dari variansi. Secara umum, variansi dirumuskun sabagai : Image(\"img/median.png\")","title":"Median"},{"location":"Tugas1/#modus","text":"Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Sedangkan data ordinal adalah data kategorik yang bisa diurutkan, misalnya kita menanyakan kepada 100 orang tentang kebiasaan untuk mencuci kaki sebelum tidur, dengan pilihan jawaban: selalu (5), sering (4), kadang-kadang(3), jarang (2), tidak pernah (1). Apabila kita ingin melihat ukuran pemusatannya lebih baik menggunakan modus yaitu yaitu jawaban yang paling banyak dipilih, misalnya sering (2). Berarti sebagian besar orang dari 100 orang yang ditanyakan menjawab sering mencuci kaki sebelum tidur. Inilah cara menghitung modus: Image(\"img/modus.png\") Data yang belum dikelompokkan Modus dari data yang belum dikelompokkan adalah ukuran yang memiliki frekuensi tertinggi. Modus dilambangkan mo. Data yang telah dikelompokkan Rumus Modus dari data yang telah dikelompokkan dihitung dengan rumus:","title":"Modus"},{"location":"Tugas1/#standar-deviasi","text":"Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual thd rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Image(\"img/std.png\")","title":"Standar deviasi"},{"location":"Tugas1/#skewness-kemiringan-atau-kecondongan","text":"Kecondongan suatu kurva dapat dilihat dari perbedaan letak mean, median dan modusnya. Jika ketiga ukuran pemusatan data tersebut berada pada titik yang sama, maka dikatakan simetris atau data berdistribusi normal. Sedangkan jika tidak berarti data tidak simetris atau tidak berdistribusi normal. Ukuran kecondongan data terbagi atas tiga bagian, yaitu : Kecondongan data ke arah kiri (condong negatif) dimana nilai modus lebih dari nilai mean (modus > mean). Kecondongan data simetris (distribusi normal) dimana nilai mean dan modus adalah sama (mean = modus). Kecondongan data ke arah kanan (condong positif) dimana nilai mean lebih dari nilai modus (mean > modus). Image(\"img/skew.png\")","title":"Skewness (kemiringan atau kecondongan)"},{"location":"Tugas1/#menampilkan-data-menggunakan-modul-modul-dari-python","text":"import pandas as pd pd.read_csv(\"tinggi.csv\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TINGGI BADAN 0 177 1 169 2 171 3 173 4 172 5 173 6 170 7 172 8 169 9 166 10 176 11 172 12 175 13 169 14 173 15 176 16 173 17 172 18 175 19 177 20 167 21 166 22 176 23 170 24 179 25 167 26 168 27 173 28 177 29 180 ... ... 70 167 71 165 72 178 73 170 74 170 75 175 76 180 77 177 78 178 79 169 80 166 81 170 82 168 83 178 84 176 85 175 86 166 87 180 88 179 89 177 90 168 91 172 92 165 93 165 94 168 95 167 96 179 97 173 98 172 99 167 100 rows \u00d7 1 columns","title":"Menampilkan data menggunakan modul-modul dari python"},{"location":"Tugas1/#menampilkan-statistik-deskriptif","text":"from scipy import stats df=pd.read_csv(\"tinggi.csv\") df['TINGGI BADAN'].describe() count 100.000000 mean 172.030000 std 4.370366 min 165.000000 25% 168.000000 50% 172.000000 75% 175.250000 max 180.000000 Name: TINGGI BADAN, dtype: float64","title":"Menampilkan statistik deskriptif"},{"location":"Tugas1/#skewness","text":"df['TINGGI BADAN'].skew() 0.11048890855968597 import seaborn as sns sns.distplot(df); C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg. warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"","title":"Skewness"},{"location":"Tugas1/#tugas","text":"Membuat data random sebanyak 4 kolom dan hitung statistik deskriptifnya. from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('data1.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Tinggi Badan (cm) Berat Badan (kg) Lebar Bahu (cm) Tk Darah (mm/Hg) 0 173 65 39 142 1 154 72 40 145 2 165 59 43 164 3 156 65 38 133 4 156 59 37 147 5 163 49 44 132 6 174 53 40 169 7 175 41 37 121 8 160 40 48 157 9 157 54 38 149 10 163 65 44 135 11 157 54 36 125 12 162 62 40 111 13 164 79 44 108 14 175 74 43 168 15 155 47 41 166 16 171 59 42 120 17 171 57 39 168 18 163 75 36 129 19 168 74 36 125 20 160 61 50 154 21 157 50 41 124 22 167 48 43 118 23 173 64 46 105 24 162 63 44 107 25 170 56 43 152 26 173 78 50 142 27 157 65 44 130 28 163 59 45 140 29 166 72 39 156 ... ... ... ... ... 70 171 68 41 101 71 165 45 47 120 72 160 61 46 130 73 166 61 45 109 74 170 74 38 103 75 173 55 35 163 76 166 63 35 137 77 166 70 38 108 78 166 60 37 120 79 171 63 47 105 80 173 76 37 104 81 159 50 44 139 82 155 75 50 168 83 169 58 46 118 84 171 41 46 108 85 161 65 42 152 86 163 43 49 130 87 161 44 40 132 88 160 56 41 119 89 163 61 45 169 90 163 63 35 153 91 154 45 46 150 92 157 58 49 147 93 158 67 47 117 94 160 52 47 102 95 167 41 44 134 96 168 66 49 142 97 161 49 38 116 98 159 65 49 161 99 175 41 46 157 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"stats\"]+[x for x in df.columns], [\"count\"]+[df[col].count() for col in df.columns], [\"mean\"]+[df[col].mean() for col in df.columns], [\"std\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min\"]+[df[col].min() for col in df.columns], [\"max\"]+[df[col].max() for col in df.columns], [\"q1\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) stats Tinggi Badan (cm) Berat Badan (kg) Lebar Bahu (cm) Tk Darah (mm/Hg) count 100 100 100 100 mean 164.98 59.53 42.19 132.15 std 6.17 10.66 4.42 19.99 min 154 40 35 101 max 175 80 50 169 q1 160.0 52.75 38.75 116.75 q2 165.5 60.0 42.5 132.0 q3 170.25 66.0 46.0 148.25 skew -0.03 -0.05 0.02 0.12","title":"Tugas"},{"location":"knn/","text":"Weight-Based K-NN","title":"Tugas 5 - Weight-Based K-NN"},{"location":"knn/#weight-based-k-nn","text":"","title":"Weight-Based K-NN"},{"location":"tugas2/","text":"Tugas 2 Data Mining : Pemahaman Data Atribut-Atribut dalam Data Mining Atribut adalah karakteristik/ ciri/ variabel yang mendeskripsikan suatu objek. Objek adalah sesuatu yang digambarkan oleh atribut. Tipe-tipe data akan menentukan tipe operasi apa yang bisa dilakukan pada data tersebut. Tipe-tipe atribut 1. Nominal Nominal adalah tipe atribut yang mengidentifikasikan suatu perbedaan sifat, yang bertujuan hanya untuk membedakan nilai yang satu dengan yang lainnya. + Contoh: + NIM mahasiswa: E1E1 10 012, E1B2 09 123, E1A1 11 011. + Merek Handphone: Nokia, LG, Sony Ericson, Samsung. 2. Ordinal Ordinal adalah tipe atribut yang mengidentifikasikan informasi mengenai suatu tingkatan yang menjadikan pembeda dari objek. Contoh: + Peringkat: Juara 1, Juara 2. + Kelas: Kelas bawah, kelas menengah, kelas atas. + Angkatan: Junior, Senior, Neneor. 3. Biner Biner adalah atribut nominal dengan hanya dua kategori atau menyatakan: 0 atau 1. 0 biasanya berarti bahwa atribut tidak ada, dan 1 berarti bahwa itu ada. Contoh: + Jenis kelamin: Pria dan wanita. + Power: On dan Off. + Penghasilan: Untung dan Rugi. + Kutub: Utara dan Selatan. 4. Numerik Numerik adalah atribut kuantitatif yang terukur, biasanya dalam bentuk tipe integer. Contoh: + Suhu: 50 Derajat Celcius, 60 derajat Kelvin, 10 derajat Fehrenheit. + Tahun: 2001,2006,2012. Mengukur Jarak Data Mengukur Jarak Data Tipe Numerik Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1,v2 menyatakan dua vektor yang menyatakan v1=x1,x2,x3,...,xn dan v2=y1,y2,y3,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas data ukuran jarak, diantaranya Minkowski Distance Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam runang dimensi n. Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. from IPython.display import Image Image(\"img/minkowski.png\") Manhattan distance Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Image(\"img/manhattan.png\") Euclidean distance Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan Image(\"img/average.png\") Chord distance Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan Image(\"img/chors.png\") def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(a.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(a.values.tolist()[v1][jnis[x]])*int(a.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) Menghitung Jarak Data Tipe Binary Image(\"img/binary.png\") def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(k.values.tolist()[v1][jnis[x]]))==2 and (int(k.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) Mengukur Jarak Data Tipe Categorical Image(\"img/categorical.png\") def categoricalDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): if (a.values.tolist()[v1][jnis[x]])!=(a.values.tolist()[v2][jnis[x]]): jmlh=jmlh+1 return (jmlh) Mengukur Jarak Data Tipe Ordinal Image(\"img/ordinal.png\") def ordDist(v1,v2,jns): jmlh=0 for x in range (len(jns)): z1=int(a.values.tolist()[v1][jns[x]])-1 z2=int(a.values.tolist()[v2][jns[x]])-1 jmlh=jmlh+chordDist(z1,z2,jns) return (jmlh) Mengukur Jarak Data Campuran Image(\"img/campuran.png\") def jarak(v1,v2): return ((chordDist(v1,v2,numerical)+ordDist(v1,v2,ordinal)+categoricalDist(v1,v2,categorical)+binaryDist(v1,v2,binary))/4) Tugas mencari dataset campuran lalu mengukur jaraknya from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('dataTugas2.csv') a=df.iloc[17:23] a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sex length diameter height whole weight shucked weight viscera weight shell weight rings 17 F 0.440 0.340 0.100 0.4510 0.1880 0.087 0.130 10.0 18 M 0.365 0.295 0.080 0.2555 0.0970 0.043 0.100 7.0 19 M 0.450 0.320 0.100 0.3810 0.1705 0.075 0.115 9.0 20 M 0.355 0.280 0.095 0.2455 0.0955 0.062 0.075 11.0 21 I 0.380 0.275 0.100 0.2255 0.0800 0.049 0.085 10.0 22 F 0.565 0.440 0.155 0.9395 0.4275 0.214 0.270 12.0 categorical=[0] num=[1,2,3,4,5,6,7,8] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1-v2\"]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0], [\"v1-v4\"]+[0]+[0]+[0], [\"v1-v5\"]+[0]+[0]+[0], [\"v1-v6\"]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1-v2 0 0 0 v1-v3 0 0 0 v1-v4 0 0 0 v1-v5 0 0 0 v1-v6 0 0 0 def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(a.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(a.values.tolist()[v1][jnis[x]])*int(a.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1,v2\"]+[0]+[\"{:.4f}\".format(chordDist(0,1,num))]+[0], [\"v1,v3\"]+[0]+[\"{:.4f}\".format(chordDist(0,2,num))]+[0], [\"v1,v4\"]+[0]+[\"{:.4f}\".format(chordDist(0,3,num))]+[0], [\"v1,v5\"]+[0]+[\"{:.4f}\".format(chordDist(0,4,num))]+[0], [\"v1,v6\"]+[0]+[\"{:.4f}\".format(chordDist(0,5,num))]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1,v2 0 1.4041 0 v1,v3 0 1.4063 0 v1,v4 0 1.4078 0 v1,v5 0 1.4071 0 v1,v6 0 1.4083 0 def ordDist(v1,v2,jns): jmlh=0 for x in range (len(jns)): z1=int(a.values.tolist()[v1][jns[x]])-1 z2=int(a.values.tolist()[v2][jns[x]])-1 jmlh=jmlh+chordDist(z1,z2,jns) return (jmlh) def categoricalDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): if (a.values.tolist()[v1][jnis[x]])!=(a.values.tolist()[v2][jnis[x]]): jmlh=jmlh+1 return (jmlh) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1,v2\"]+[0]+[\"{:.4f}\".format(chordDist(0,1,num))]+[categoricalDist(0,1,categorical)], [\"v1,v3\"]+[0]+[\"{:.4f}\".format(chordDist(0,2,num))]+[categoricalDist(0,2,categorical)], [\"v1,v4\"]+[0]+[\"{:.4f}\".format(chordDist(0,3,num))]+[categoricalDist(0,3,categorical)], [\"v1,v5\"]+[0]+[\"{:.4f}\".format(chordDist(0,4,num))]+[categoricalDist(0,4,categorical)], [\"v1,v6\"]+[0]+[\"{:.4f}\".format(chordDist(0,5,num))]+[categoricalDist(0,5,categorical)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1,v2 0 1.4041 1 v1,v3 0 1.4063 1 v1,v4 0 1.4078 1 v1,v5 0 1.4071 1 v1,v6 0 1.4083 0 def campuranDist(v1,v2): return ((chordDist(v1,v2,num)+categoricalDist(v1,v2,categorical))/2) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1,v2\"]+[\"{:.4f}\".format(campuranDist(0,1))]+[\"{:.4f}\".format(chordDist(0,1,num))]+[categoricalDist(0,1,categorical)], [\"v1,v3\"]+[\"{:.4f}\".format(campuranDist(0,2))]+[\"{:.4f}\".format(chordDist(0,2,num))]+[categoricalDist(0,2,categorical)], [\"v1,v4\"]+[\"{:.4f}\".format(campuranDist(0,3))]+[\"{:.4f}\".format(chordDist(0,3,num))]+[categoricalDist(0,3,categorical)], [\"v1,v5\"]+[\"{:.4f}\".format(campuranDist(0,4))]+[\"{:.4f}\".format(chordDist(0,4,num))]+[categoricalDist(0,4,categorical)], [\"v1,v6\"]+[\"{:.4f}\".format(campuranDist(0,5))]+[\"{:.4f}\".format(chordDist(0,5,num))]+[categoricalDist(0,5,categorical)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1,v2 1.2020 1.4041 1 v1,v3 1.2032 1.4063 1 v1,v4 1.2039 1.4078 1 v1,v5 1.2036 1.4071 1 v1,v6 0.7042 1.4083 0","title":"Tugas 2 - Pemahaman Data"},{"location":"tugas2/#tugas-2-data-mining-pemahaman-data","text":"","title":"Tugas 2 Data Mining : Pemahaman Data"},{"location":"tugas2/#atribut-atribut-dalam-data-mining","text":"Atribut adalah karakteristik/ ciri/ variabel yang mendeskripsikan suatu objek. Objek adalah sesuatu yang digambarkan oleh atribut. Tipe-tipe data akan menentukan tipe operasi apa yang bisa dilakukan pada data tersebut.","title":"Atribut-Atribut dalam Data Mining"},{"location":"tugas2/#tipe-tipe-atribut","text":"1. Nominal Nominal adalah tipe atribut yang mengidentifikasikan suatu perbedaan sifat, yang bertujuan hanya untuk membedakan nilai yang satu dengan yang lainnya. + Contoh: + NIM mahasiswa: E1E1 10 012, E1B2 09 123, E1A1 11 011. + Merek Handphone: Nokia, LG, Sony Ericson, Samsung. 2. Ordinal Ordinal adalah tipe atribut yang mengidentifikasikan informasi mengenai suatu tingkatan yang menjadikan pembeda dari objek. Contoh: + Peringkat: Juara 1, Juara 2. + Kelas: Kelas bawah, kelas menengah, kelas atas. + Angkatan: Junior, Senior, Neneor. 3. Biner Biner adalah atribut nominal dengan hanya dua kategori atau menyatakan: 0 atau 1. 0 biasanya berarti bahwa atribut tidak ada, dan 1 berarti bahwa itu ada. Contoh: + Jenis kelamin: Pria dan wanita. + Power: On dan Off. + Penghasilan: Untung dan Rugi. + Kutub: Utara dan Selatan. 4. Numerik Numerik adalah atribut kuantitatif yang terukur, biasanya dalam bentuk tipe integer. Contoh: + Suhu: 50 Derajat Celcius, 60 derajat Kelvin, 10 derajat Fehrenheit. + Tahun: 2001,2006,2012.","title":"Tipe-tipe atribut"},{"location":"tugas2/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"tugas2/#mengukur-jarak-data-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1,v2 menyatakan dua vektor yang menyatakan v1=x1,x2,x3,...,xn dan v2=y1,y2,y3,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas data ukuran jarak, diantaranya","title":"Mengukur Jarak Data Tipe Numerik"},{"location":"tugas2/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam runang dimensi n. Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. from IPython.display import Image Image(\"img/minkowski.png\")","title":"Minkowski Distance"},{"location":"tugas2/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Image(\"img/manhattan.png\")","title":"Manhattan distance"},{"location":"tugas2/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"tugas2/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan Image(\"img/average.png\")","title":"Average Distance"},{"location":"tugas2/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan Image(\"img/chors.png\") def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(a.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(a.values.tolist()[v1][jnis[x]])*int(a.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5)","title":"Chord distance"},{"location":"tugas2/#menghitung-jarak-data-tipe-binary","text":"Image(\"img/binary.png\") def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(k.values.tolist()[v1][jnis[x]]))==2 and (int(k.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t))","title":"Menghitung Jarak Data Tipe Binary"},{"location":"tugas2/#mengukur-jarak-data-tipe-categorical","text":"Image(\"img/categorical.png\") def categoricalDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): if (a.values.tolist()[v1][jnis[x]])!=(a.values.tolist()[v2][jnis[x]]): jmlh=jmlh+1 return (jmlh)","title":"Mengukur Jarak Data Tipe Categorical"},{"location":"tugas2/#mengukur-jarak-data-tipe-ordinal","text":"Image(\"img/ordinal.png\") def ordDist(v1,v2,jns): jmlh=0 for x in range (len(jns)): z1=int(a.values.tolist()[v1][jns[x]])-1 z2=int(a.values.tolist()[v2][jns[x]])-1 jmlh=jmlh+chordDist(z1,z2,jns) return (jmlh)","title":"Mengukur Jarak Data Tipe Ordinal"},{"location":"tugas2/#mengukur-jarak-data-campuran","text":"Image(\"img/campuran.png\") def jarak(v1,v2): return ((chordDist(v1,v2,numerical)+ordDist(v1,v2,ordinal)+categoricalDist(v1,v2,categorical)+binaryDist(v1,v2,binary))/4)","title":"Mengukur Jarak Data Campuran"},{"location":"tugas2/#tugas","text":"mencari dataset campuran lalu mengukur jaraknya from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('dataTugas2.csv') a=df.iloc[17:23] a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sex length diameter height whole weight shucked weight viscera weight shell weight rings 17 F 0.440 0.340 0.100 0.4510 0.1880 0.087 0.130 10.0 18 M 0.365 0.295 0.080 0.2555 0.0970 0.043 0.100 7.0 19 M 0.450 0.320 0.100 0.3810 0.1705 0.075 0.115 9.0 20 M 0.355 0.280 0.095 0.2455 0.0955 0.062 0.075 11.0 21 I 0.380 0.275 0.100 0.2255 0.0800 0.049 0.085 10.0 22 F 0.565 0.440 0.155 0.9395 0.4275 0.214 0.270 12.0 categorical=[0] num=[1,2,3,4,5,6,7,8] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1-v2\"]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0], [\"v1-v4\"]+[0]+[0]+[0], [\"v1-v5\"]+[0]+[0]+[0], [\"v1-v6\"]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1-v2 0 0 0 v1-v3 0 0 0 v1-v4 0 0 0 v1-v5 0 0 0 v1-v6 0 0 0 def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(a.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(a.values.tolist()[v1][jnis[x]])*int(a.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1,v2\"]+[0]+[\"{:.4f}\".format(chordDist(0,1,num))]+[0], [\"v1,v3\"]+[0]+[\"{:.4f}\".format(chordDist(0,2,num))]+[0], [\"v1,v4\"]+[0]+[\"{:.4f}\".format(chordDist(0,3,num))]+[0], [\"v1,v5\"]+[0]+[\"{:.4f}\".format(chordDist(0,4,num))]+[0], [\"v1,v6\"]+[0]+[\"{:.4f}\".format(chordDist(0,5,num))]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1,v2 0 1.4041 0 v1,v3 0 1.4063 0 v1,v4 0 1.4078 0 v1,v5 0 1.4071 0 v1,v6 0 1.4083 0 def ordDist(v1,v2,jns): jmlh=0 for x in range (len(jns)): z1=int(a.values.tolist()[v1][jns[x]])-1 z2=int(a.values.tolist()[v2][jns[x]])-1 jmlh=jmlh+chordDist(z1,z2,jns) return (jmlh) def categoricalDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): if (a.values.tolist()[v1][jnis[x]])!=(a.values.tolist()[v2][jnis[x]]): jmlh=jmlh+1 return (jmlh) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1,v2\"]+[0]+[\"{:.4f}\".format(chordDist(0,1,num))]+[categoricalDist(0,1,categorical)], [\"v1,v3\"]+[0]+[\"{:.4f}\".format(chordDist(0,2,num))]+[categoricalDist(0,2,categorical)], [\"v1,v4\"]+[0]+[\"{:.4f}\".format(chordDist(0,3,num))]+[categoricalDist(0,3,categorical)], [\"v1,v5\"]+[0]+[\"{:.4f}\".format(chordDist(0,4,num))]+[categoricalDist(0,4,categorical)], [\"v1,v6\"]+[0]+[\"{:.4f}\".format(chordDist(0,5,num))]+[categoricalDist(0,5,categorical)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1,v2 0 1.4041 1 v1,v3 0 1.4063 1 v1,v4 0 1.4078 1 v1,v5 0 1.4071 1 v1,v6 0 1.4083 0 def campuranDist(v1,v2): return ((chordDist(v1,v2,num)+categoricalDist(v1,v2,categorical))/2) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Categorical\"], [\"v1,v2\"]+[\"{:.4f}\".format(campuranDist(0,1))]+[\"{:.4f}\".format(chordDist(0,1,num))]+[categoricalDist(0,1,categorical)], [\"v1,v3\"]+[\"{:.4f}\".format(campuranDist(0,2))]+[\"{:.4f}\".format(chordDist(0,2,num))]+[categoricalDist(0,2,categorical)], [\"v1,v4\"]+[\"{:.4f}\".format(campuranDist(0,3))]+[\"{:.4f}\".format(chordDist(0,3,num))]+[categoricalDist(0,3,categorical)], [\"v1,v5\"]+[\"{:.4f}\".format(campuranDist(0,4))]+[\"{:.4f}\".format(chordDist(0,4,num))]+[categoricalDist(0,4,categorical)], [\"v1,v6\"]+[\"{:.4f}\".format(campuranDist(0,5))]+[\"{:.4f}\".format(chordDist(0,5,num))]+[categoricalDist(0,5,categorical)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Categorical v1,v2 1.2020 1.4041 1 v1,v3 1.2032 1.4063 1 v1,v4 1.2039 1.4078 1 v1,v5 1.2036 1.4071 1 v1,v6 0.7042 1.4083 0","title":"Tugas"},{"location":"tugas3/","text":"Tugas 3 Data Mining : Seleksi Fitur Pengertian Seleksi Fitur Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. contoh data permasalahan cuaca from IPython.display import Image import pandas as pd import math df=pd.read_csv('feature selection1.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } outlook temperature humidity windy play 0 sunny hot high False no 1 sunny hot high True no 2 overcast hot high False yes 3 rainy mild high False yes 4 rainy cool normal False yes 5 rainy cool normal True no 6 overcast cool normal True yes 7 sunny mild high False no 8 sunny cool normal False yes 9 rainy mild normal False yes 10 sunny mild normal True yes 11 overcast mild high True yes 12 overcast hot normal False yes 13 rainy mild high True no Mencari Entropy Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S): Image(\"img/entropy.png\") dimana : T = ruang sampel data yang di gunakaan untuk data pelatihan Pi = Probability muncul dalam row # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy) ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] # menentukan count value pada jenis tiap atribut def countvJenis(jenis,kelas,kolomJenis,kolomKelas,data): hasil=[] for x in range (len(kelas)): hasil.append(0) for i in range (len(data)): if data.values.tolist()[i][kolomJenis] == jenis: for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil def t_list(atribut,n): temp=[] for i in range(len(atribut)): temp.append(countvJenis(atribut[i],kelas,df.shape[1]-n,df.shape[1]-1,df)) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(tOutlook, tTemp, tHum, tWin) [[3, 2], [0, 4], [2, 3]] [[2, 2], [2, 4], [1, 3]] [[4, 3], [1, 6]] [[2, 6], [3, 3]] # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp eOutlook=e_list(outlook,5) eTemp=e_list(temp,4) eHum=e_list(humidity,3) eWin=e_list(windy,2) print(eOutlook, eTemp, eHum, eWin) [0.9709505944546686, 0.0, 0.9709505944546686] [1.0, 0.9182958340544896, 0.8112781244591328] [0.9852281360342516, 0.5916727785823275] [0.8112781244591328, 1.0] Gain Gain adalah sebuah fitur yang terdapat pada sebuah data , untuk menghitungnya contoh rumusnya : Image(\"img/gain.png\") dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur def gain(T_list,E_list,pKelas,data): txv=0 for i in range (len(E_list)): jumlah=0 for y in range (len(T_list[i])): jumlah+=T_list[i][y] txv+=jumlah/(len(data))*(E_list[i]) eUtama=entropy(pKelas) hasil=eUtama-txv return hasil Hasil Keseluruhan Data from IPython.display import HTML, display import tabulate table=[ [\"attributes\"]+[\"Jenis\"]+[\"no\"]+[\"yes\"]+[\"entropy\"]+[\"gain\"], [\"OUTLOOK\"]+[\"Sunny\"]+[tOutlook[0][0]]+[tOutlook[0][1]]+[eOutlook[0]]+[gain(tOutlook,eOutlook,pKelas,df)], [\"\"]+[\"Overcast\"]+[tOutlook[1][0]]+[tOutlook[1][1]]+[eOutlook[1]]+[], [\"\"]+[\"Rainy\"]+[tOutlook[2][0]]+[tOutlook[2][1]]+[eOutlook[2]]+[], [\"TEMPERATURE\"]+[\"Hot\"]+[tTemp[0][0]]+[tTemp[0][1]]+[eTemp[0]]+[gain(tTemp,eTemp,pKelas,df)], [\"\"]+[\"Mild\"]+[tTemp[1][0]]+[tTemp[1][1]]+[eTemp[1]]+[], [\"\"]+[\"Cool\"]+[tTemp[2][0]]+[tTemp[2][1]]+[eTemp[2]]+[], [\"HUMIDITY\"]+[\"High\"]+[tHum[0][0]]+[tHum[0][1]]+[eHum[0]]+[gain(tHum,eHum,pKelas,df)], [\"\"]+[\"Normal\"]+[tHum[1][0]]+[tHum[1][1]]+[eHum[1]]+[], [\"WINDY\"]+[\"False\"]+[tWin[0][0]]+[tWin[0][1]]+[eWin[0]]+[gain(tWin,eWin,pKelas,df)], [\"\"]+[\"True\"]+[tWin[1][0]]+[tWin[1][1]]+[eWin[1]]+[], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) attributes Jenis no yes entropy gain OUTLOOK Sunny 3 2 0.9709505944546686 0.2467498197744391 Overcast 0 4 0.0 Rainy 2 3 0.9709505944546686 TEMPERATURE Hot 2 2 1.0 0.029222565658954647 Mild 2 4 0.9182958340544896 Cool 1 3 0.8112781244591328 HUMIDITY High 4 3 0.9852281360342516 0.15183550136234136 Normal 1 6 0.5916727785823275 WINDY False 2 6 0.8112781244591328 0.04812703040826927 True 3 3 1.0 Rank Gain Score Skor ini sudah di urutkan menurut tingkat tertingi dari sebuat semua Gain, tergantung anda ingin mengambil berapa banyak data yang terpenting , bisa 2 atau lebih. table=[ [\"attributes\"]+[\"Gain Score\"], [\"OUTLOOK\"]+[gain(tOutlook,eOutlook,pKelas,df)], [\"HUMIDITY\"]+[gain(tHum,eHum,pKelas,df)], [\"WINDY\"]+[gain(tWin,eWin,pKelas,df)], [\"TEMPERATURE\"]+[gain(tTemp,eTemp,pKelas,df)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) attributes Gain Score OUTLOOK 0.2467498197744391 HUMIDITY 0.15183550136234136 WINDY 0.04812703040826927 TEMPERATURE 0.029222565658954647","title":"Tugas 3 - Seleksi Fitur"},{"location":"tugas3/#tugas-3-data-mining-seleksi-fitur","text":"","title":"Tugas 3 Data Mining : Seleksi Fitur"},{"location":"tugas3/#pengertian-seleksi-fitur","text":"Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. contoh data permasalahan cuaca from IPython.display import Image import pandas as pd import math df=pd.read_csv('feature selection1.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } outlook temperature humidity windy play 0 sunny hot high False no 1 sunny hot high True no 2 overcast hot high False yes 3 rainy mild high False yes 4 rainy cool normal False yes 5 rainy cool normal True no 6 overcast cool normal True yes 7 sunny mild high False no 8 sunny cool normal False yes 9 rainy mild normal False yes 10 sunny mild normal True yes 11 overcast mild high True yes 12 overcast hot normal False yes 13 rainy mild high True no","title":"Pengertian Seleksi Fitur"},{"location":"tugas3/#mencari-entropy","text":"Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S): Image(\"img/entropy.png\") dimana : T = ruang sampel data yang di gunakaan untuk data pelatihan Pi = Probability muncul dalam row # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy) ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] # menentukan count value pada jenis tiap atribut def countvJenis(jenis,kelas,kolomJenis,kolomKelas,data): hasil=[] for x in range (len(kelas)): hasil.append(0) for i in range (len(data)): if data.values.tolist()[i][kolomJenis] == jenis: for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil def t_list(atribut,n): temp=[] for i in range(len(atribut)): temp.append(countvJenis(atribut[i],kelas,df.shape[1]-n,df.shape[1]-1,df)) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(tOutlook, tTemp, tHum, tWin) [[3, 2], [0, 4], [2, 3]] [[2, 2], [2, 4], [1, 3]] [[4, 3], [1, 6]] [[2, 6], [3, 3]] # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp eOutlook=e_list(outlook,5) eTemp=e_list(temp,4) eHum=e_list(humidity,3) eWin=e_list(windy,2) print(eOutlook, eTemp, eHum, eWin) [0.9709505944546686, 0.0, 0.9709505944546686] [1.0, 0.9182958340544896, 0.8112781244591328] [0.9852281360342516, 0.5916727785823275] [0.8112781244591328, 1.0]","title":"Mencari Entropy"},{"location":"tugas3/#gain","text":"Gain adalah sebuah fitur yang terdapat pada sebuah data , untuk menghitungnya contoh rumusnya : Image(\"img/gain.png\") dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur def gain(T_list,E_list,pKelas,data): txv=0 for i in range (len(E_list)): jumlah=0 for y in range (len(T_list[i])): jumlah+=T_list[i][y] txv+=jumlah/(len(data))*(E_list[i]) eUtama=entropy(pKelas) hasil=eUtama-txv return hasil","title":"Gain"},{"location":"tugas3/#hasil-keseluruhan-data","text":"from IPython.display import HTML, display import tabulate table=[ [\"attributes\"]+[\"Jenis\"]+[\"no\"]+[\"yes\"]+[\"entropy\"]+[\"gain\"], [\"OUTLOOK\"]+[\"Sunny\"]+[tOutlook[0][0]]+[tOutlook[0][1]]+[eOutlook[0]]+[gain(tOutlook,eOutlook,pKelas,df)], [\"\"]+[\"Overcast\"]+[tOutlook[1][0]]+[tOutlook[1][1]]+[eOutlook[1]]+[], [\"\"]+[\"Rainy\"]+[tOutlook[2][0]]+[tOutlook[2][1]]+[eOutlook[2]]+[], [\"TEMPERATURE\"]+[\"Hot\"]+[tTemp[0][0]]+[tTemp[0][1]]+[eTemp[0]]+[gain(tTemp,eTemp,pKelas,df)], [\"\"]+[\"Mild\"]+[tTemp[1][0]]+[tTemp[1][1]]+[eTemp[1]]+[], [\"\"]+[\"Cool\"]+[tTemp[2][0]]+[tTemp[2][1]]+[eTemp[2]]+[], [\"HUMIDITY\"]+[\"High\"]+[tHum[0][0]]+[tHum[0][1]]+[eHum[0]]+[gain(tHum,eHum,pKelas,df)], [\"\"]+[\"Normal\"]+[tHum[1][0]]+[tHum[1][1]]+[eHum[1]]+[], [\"WINDY\"]+[\"False\"]+[tWin[0][0]]+[tWin[0][1]]+[eWin[0]]+[gain(tWin,eWin,pKelas,df)], [\"\"]+[\"True\"]+[tWin[1][0]]+[tWin[1][1]]+[eWin[1]]+[], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) attributes Jenis no yes entropy gain OUTLOOK Sunny 3 2 0.9709505944546686 0.2467498197744391 Overcast 0 4 0.0 Rainy 2 3 0.9709505944546686 TEMPERATURE Hot 2 2 1.0 0.029222565658954647 Mild 2 4 0.9182958340544896 Cool 1 3 0.8112781244591328 HUMIDITY High 4 3 0.9852281360342516 0.15183550136234136 Normal 1 6 0.5916727785823275 WINDY False 2 6 0.8112781244591328 0.04812703040826927 True 3 3 1.0","title":"Hasil Keseluruhan Data"},{"location":"tugas3/#rank-gain-score","text":"Skor ini sudah di urutkan menurut tingkat tertingi dari sebuat semua Gain, tergantung anda ingin mengambil berapa banyak data yang terpenting , bisa 2 atau lebih. table=[ [\"attributes\"]+[\"Gain Score\"], [\"OUTLOOK\"]+[gain(tOutlook,eOutlook,pKelas,df)], [\"HUMIDITY\"]+[gain(tHum,eHum,pKelas,df)], [\"WINDY\"]+[gain(tWin,eWin,pKelas,df)], [\"TEMPERATURE\"]+[gain(tTemp,eTemp,pKelas,df)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) attributes Gain Score OUTLOOK 0.2467498197744391 HUMIDITY 0.15183550136234136 WINDY 0.04812703040826927 TEMPERATURE 0.029222565658954647","title":"Rank Gain Score"}]}